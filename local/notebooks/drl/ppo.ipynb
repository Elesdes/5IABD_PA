{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 17:36:29.269943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 17:36:31.192965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 17:36:35.274026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-30 17:36:35.274619: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.log_probability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, log_probability):\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.log_probability_buffer[self.pointer] = log_probability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.log_probability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def log_probabilities(logits, a):\n",
    "    log_probabilities_all = keras.ops.log_softmax(logits)\n",
    "    log_probability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * log_probabilities_all, axis=1\n",
    "    )\n",
    "    return log_probability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, log_probability_buffer, advantage_buffer\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        ratio = keras.ops.exp(\n",
    "            log_probabilities(actor(observation_buffer), action_buffer)\n",
    "            - log_probability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        log_probability_buffer\n",
    "        - log_probabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 4000\n",
    "epochs = 10\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "# If you want to see the environment, set render to True\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if render:\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    temp_dir = \"frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "else:\n",
    "    gym.make(\"CartPole-v1\")\n",
    "\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Mean Return: 23.391812865497077 | Mean Length: 23.391812865497077\n",
      "Epoch: 2 | Mean Return: 28.776978417266186 | Mean Length: 28.776978417266186\n",
      "Epoch: 3 | Mean Return: 43.47826086956522 | Mean Length: 43.47826086956522\n",
      "Epoch: 4 | Mean Return: 68.96551724137932 | Mean Length: 68.96551724137932\n",
      "Epoch: 5 | Mean Return: 97.5609756097561 | Mean Length: 97.5609756097561\n",
      "Epoch: 6 | Mean Return: 133.33333333333334 | Mean Length: 133.33333333333334\n",
      "Epoch: 7 | Mean Return: 160.0 | Mean Length: 160.0\n",
      "Epoch: 8 | Mean Return: 200.0 | Mean Length: 200.0\n",
      "Epoch: 9 | Mean Return: 166.66666666666666 | Mean Length: 166.66666666666666\n",
      "Epoch: 10 | Mean Return: 285.7142857142857 | Mean Length: 285.7142857142857\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            frame = env.render()\n",
    "            frame_path = os.path.join(temp_dir, f\"epoch_{epoch}_frame_{t+1:04d}.png\")\n",
    "            imageio.imwrite(frame_path, frame)\n",
    "\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        value_t = critic(observation)\n",
    "        log_probability_t = log_probabilities(logits, action)\n",
    "\n",
    "        buffer.store(observation, action, reward, value_t, log_probability_t)\n",
    "\n",
    "        observation = observation_new\n",
    "\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        log_probability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, log_probability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} | Mean Return: {sum_return / num_episodes} | Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video Of Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/44000 [00:00<?, ?it/s]/tmp/ipykernel_1143/3149173939.py:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  video.append_data(imageio.imread(filename))\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "  0%|          | 1/44000 [00:00<3:01:04,  4.05it/s][swscaler @ 0x729b980] Warning: data is not aligned! This can lead to a speed loss\n",
      "100%|██████████| 44000/44000 [14:58<00:00, 49.00it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Optionally, clean up by removing the images if no longer needed\\nfor filename in filenames:\\n    os.remove(filename)\\nos.rmdir(temp_dir)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "filenames = sorted(\n",
    "    [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith(\".png\")]\n",
    ")\n",
    "with imageio.get_writer(\"training_video.mp4\", fps=30) as video:\n",
    "    for filename in tqdm(filenames):\n",
    "        video.append_data(imageio.imread(filename))\n",
    "\n",
    "\"\"\"# Optionally, clean up by removing the images if no longer needed\n",
    "for filename in filenames:\n",
    "    os.remove(filename)\n",
    "os.rmdir(temp_dir)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICARUS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
